{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flask app imports\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_pymongo import PyMongo, MongoClient\n",
    "from flask_cors import CORS\n",
    "\n",
    "# TFIDF imports\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse.csr import csr_matrix  # need this if you want to save tfidf_matrix\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Speech-to-Text file import\n",
    "\n",
    "\n",
    "top5AllFiles = []\n",
    "listFiles = []\n",
    "listwords = []\n",
    "listweights = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fullData = []\n",
    "fullData2 = []\n",
    "path = os.getcwd()\n",
    "\n",
    "\n",
    "def top5words():\n",
    "        # path is that of the current directory\n",
    "        # path = os.getcwd()\n",
    "        # print(location)\n",
    "        ps = PorterStemmer()\n",
    "        # empty list of corpus\n",
    "        corpus = []\n",
    "\n",
    "        # append each file with .txt extension to the corpus\n",
    "\n",
    "        for filename in sorted(glob.glob(os.path.join(path, '*.txt'))):\n",
    "            with open(filename, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "\n",
    "        vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # ps = PorterStemmer()\n",
    "        analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "        # lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # def stem_words(doc):\n",
    "        # return[port.stem(word) for word in analyzer(doc) if word not in stop_words]\n",
    "\n",
    "        def stem_words(doc):\n",
    "            return [lemmatizer.lemmatize(word) for word in analyzer(doc) if word not in stop_words]\n",
    "\n",
    "        cv = CountVectorizer(analyzer=stem_words, stop_words='english', lowercase=True)\n",
    "\n",
    "        # cv=CountVectorizer(analyzer='word', stop_words = 'english',lowercase=True)\n",
    "\n",
    "        # this steps generates word counts for the words in your docs\n",
    "        word_count_vector = cv.fit_transform(corpus)\n",
    "\n",
    "        word_count_vector.shape\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "        count_vector = cv.transform(corpus)\n",
    "        tf_idf_vector = tfidf_transformer.transform(count_vector)\n",
    "\n",
    "        feature_names = cv.get_feature_names()\n",
    "\n",
    "        #top5AllFiles = []\n",
    "\n",
    "        # build dataframe of first document. Determined by the index od tf-idf_vector below\n",
    "\n",
    "        corpusLength = len(corpus)\n",
    "\n",
    "        for i in range(0, corpusLength):\n",
    "            # print(i)\n",
    "            df = pd.DataFrame(tf_idf_vector[i].T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "            df.sort_values(by=[\"tfidf\"], ascending=False)\n",
    "            # get top 5 words\n",
    "            top5 = df.nlargest(5, \"tfidf\")\n",
    "            # print(top5)\n",
    "            array = []\n",
    "            data1 = []\n",
    "            for i, j in top5.iterrows():\n",
    "                data1.append(i)\n",
    "                data1.append(j.tfidf)\n",
    "\n",
    "            array.append(data1)\n",
    "            top5AllFiles.append(array)\n",
    "\n",
    "        # print(top5AllFiles)\n",
    "        # print()\n",
    "\n",
    "\n",
    "def time_data(filename, listwords):\n",
    "        tempfullData = []\n",
    "        foundWords = []\n",
    "        # try:\n",
    "        wordPresent = False\n",
    "        file = open(filename, \"r\")\n",
    "        read = file.readlines()\n",
    "        file.close()\n",
    "        linkval = \"\"\n",
    "        linktemp = []\n",
    "        link = []\n",
    "\n",
    "        for word in listwords:\n",
    "            lower = word.lower()\n",
    "            count = 0\n",
    "            for sentence in read:\n",
    "                line = sentence.split()\n",
    "                for each in line:\n",
    "                    filesName = []\n",
    "                    line2 = each.lower()\n",
    "                    line2 = line2.strip(\"!@#$%^&*(()_+=)\")\n",
    "                    line3 = line2.split(\":\")\n",
    "                    topword = []\n",
    "                    comparedword = line3[1].lower()\n",
    "                    stemmedWord = lemmatizer.lemmatize(comparedword)\n",
    "                    # print(comparedword,\":\",stemmedWord)\n",
    "                    # if line3[0]==\"word\" and lower == line3[1].lower() and lower not in foundWords:\n",
    "                    if line3[0] == \"word\" and lower == stemmedWord and lower not in foundWords:\n",
    "                        temptopword = []\n",
    "                        name = lower\n",
    "                        startTime = line[1]\n",
    "                        start = startTime.split(\"start_time:\")\n",
    "                        startFin = start[1]\n",
    "                        # print(k[1])\n",
    "                        endTime = line[2]\n",
    "                        end = endTime.split(\"end_time:\")\n",
    "                        endFin = end[1]\n",
    "                        # print(endTime.split(\"end_time:\"))\n",
    "                        val2 = line3[1]\n",
    "                        foundWords.append(lower)\n",
    "                        topword = str(lower) + \", \" + str(startFin) + \" , \" +   str(endFin)\n",
    "                        temptopword.append(topword)\n",
    "                        # tempfullData = tempfullData + temptopword\n",
    "                        tempfullData.append(temptopword)\n",
    "                        wordPresent = True\n",
    "                        filesName.append(filename)\n",
    "                        # print(filename)\n",
    "                        # print(tempfullData)\n",
    "                        # print()\n",
    "                    if line3[0] == \"link\" and wordPresent == True and line[0] not in link:\n",
    "                        link.append(line[0])\n",
    "                    if (len(tempfullData) >= 5 and filesName not in listFiles and len(filesName) != 0):\n",
    "                        listFiles.append(filesName)\n",
    "\n",
    "        # if(len(tempfullData) <5 and len(tempfullData) >0):\n",
    "        # continue\n",
    "        # print(tempfullData)\n",
    "        # print(listFiles)\n",
    "        # print()\n",
    "        tempfullData = tempfullData + link\n",
    "        if (len(tempfullData) <= 2):\n",
    "            tempfullData = []\n",
    "        if (len(tempfullData) < 5):\n",
    "            tempfullData = []\n",
    "        fullData.append(tempfullData)\n",
    "\n",
    "        # tempfullData = tempfullData + filesName\n",
    "        # fullData.append(tempfullData)\n",
    "\n",
    "\n",
    "def weights():\n",
    "    for i in top5AllFiles:\n",
    "        size = len(i[0])\n",
    "        temp = []\n",
    "        tempfloat = []\n",
    "        for j in range(0, size):\n",
    "            tempVar = []\n",
    "            tempWeight = []\n",
    "            if (j % 2 == 0):\n",
    "                weight = i[0][j + 1]\n",
    "                tempVar.append(i[0][j])\n",
    "                tempWeight.append(i[0][j + 1])\n",
    "                temp = temp + tempVar\n",
    "                tempfloat = tempfloat + tempWeight\n",
    "        listwords.append(temp)\n",
    "        listweights.append(tempfloat)\n",
    "\n",
    "\n",
    "def words_time():\n",
    "    for i in listwords:\n",
    "        # parse through file and get time stamp\n",
    "        for filename in sorted(glob.glob(os.path.join(path, '*.csv'))):\n",
    "            # print(filename)\n",
    "            time_data(filename, i)\n",
    "\n",
    "    # stip empty list within a list\n",
    "    fullData2 = [e for e in fullData if e]\n",
    "    # print(fullData2)\n",
    "    # variable definition\n",
    "    dictCorpus = {}\n",
    "    # print(listFiles)\n",
    "\n",
    "\n",
    "def words_weight():\n",
    "    sizeI = len(fullData2[0])\n",
    "    count = 0\n",
    "    lengthLimit = len(listweights)\n",
    "\n",
    "    dictCorpus = {}\n",
    "    counter = 0\n",
    "    incr = 0\n",
    "    count = 0\n",
    "    index = 0\n",
    "\n",
    "    fulLeng = len(fullData2)\n",
    "    for i in fullData2:\n",
    "        indexIn = 0\n",
    "        incr = 0\n",
    "        myDict = []\n",
    "        test1 = []\n",
    "        line = []\n",
    "        for j in i:\n",
    "            if (count < lengthLimit and incr < sizeI):\n",
    "                # tester.append(listweights[0][incr])\n",
    "                # print(j[0])\n",
    "                weight = \" ,\" + '%.3f' % (listweights[count][incr])\n",
    "                # print(weight)\n",
    "                j[0] += weight\n",
    "                incr += 1\n",
    "            count += 1\n",
    "\n",
    "    print()\n",
    "    print(fullData2)\n",
    "    print(\"corpus: $$$$$$$$$$$$$$$$$$############\")\n",
    "    # print(dictCorpus)\n",
    "    # print(listFiles)\n",
    "\n",
    "    # store dictionary in json file\n",
    "    # with open('top5Words.json', 'w') as filehandle:\n",
    "    # json.dump(dictCorpus, filehandle)\n",
    "    # store json format in database\n",
    "    # json.dump(dictCorpus,sort_keys= True,indent = 5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
